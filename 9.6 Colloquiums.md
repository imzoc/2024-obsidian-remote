---
dg-publish: true
---
[[Colloquiums]]

# [[Bo Liu]] presentation on trustworthy AI

* {tree, graph, etc} of thought.
* Clear tasks, clear deeds.
* Neuro-symbolic skills
* Variance reflects volatility reflects risk?
* Reach out to Bo.

# HPC talk
* Basically irrelevant

# Sandiway! (SMT)
* Assumption of generative grammar
* Phrase-structure grammar doesn't work.
* PSG encodes order, which is a mistake because it depends on the language...
* Efficient parsing algorithm became standard in the research paradigm, even though it wasn't even descriptive.
* Shuffling things around makes for simpler descriptions... (sure)
	* Even at the morpheme level
* Passive transformation: generate all the tokens, then shuffle them (deterministically?)
* Important: generate the structure of the sentence, as well as the sentence itself.
* No known efficient parsing algorithm... there can't be one?
	* Maybe that makes sense!
	* Maybe that's an evolved adaptatation.
* FSA: simple system, complicated descriptions. Not probably right.
SMT:
* Simple mechanism (evolutionary plausibility)
* Computational efficiency (humans have *slow wetware*)
* Simplicity of description is possible (source: Sandiway believes it's possible).
* Internal and external merge:
genetic...ally
* Merge has language-organ constraints.
* Theta-theory
* Is "modern man" so modern?
* There was an explosion of symbolic activity.

Why do we collect so much data if we throw most of it away?
Linguistics is computer science? No, computer science is linguistics.

Basic property of language
* The brain builds language structure and then chooses the "nearest" based on the structure, not on the linear order.

language is designed to construct thoughts efficiently???
Not to communicate efficiently!

I really really like merge.
Merge has no labels!!!

9/24 Zach