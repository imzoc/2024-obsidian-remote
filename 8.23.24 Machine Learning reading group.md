---
dg-publish: true

---
[[Colloquiums]]

[Class incremental learning (CIL)](https://arxiv.org/pdf/2407.14143v1)

Purpose: learn new classes over time while retaining old knowledge

Challenge: catastrophic forgetting when adding new classes.

Special features:
* During training for new data, we measure the influence of new classes on old ones and adjust the representations, using textual features.
* After training, we employ a decomposed parameter fusion to further mitigate forgetting during adapter module fine-tuning.

Components of the model:
* Pre-trained CLIP encoder and adapter
* Cross-entropy loss to fine-tune the adapter
* Hinge loss, idk what it is

centroid = mean

Covariance matrix to understand feature variability

Thoughts prompted by this paper/presentation:
* I think taking a statistical learning class would be great.
* Practice using and understanding trained matrix parameters in deep learning architectures.
* This guy with the hat keeps saying "my intuition" which is freaking awesome
* He compared the method to human capability!
